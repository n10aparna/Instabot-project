{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e473ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Disable Chrome Browser Notification\n",
    "option = Options()\n",
    "option.add_argument(\"--disable-infobars\")\n",
    "option.add_argument(\"start-maximized\")\n",
    "option.add_argument(\"--disable-extensions\")\n",
    "option.add_experimental_option(\"prefs\", { \n",
    "    \"profile.default_content_setting_values.notifications\": 1 \n",
    "})\n",
    "driver = webdriver.Chrome(options=option)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25899c5c",
   "metadata": {},
   "source": [
    "Q.1 Login to your Instagram Handle\n",
    "    1.Submit with sample username and password\n",
    "  \n",
    "Answer: Here I have visited the site and then I used implict wait to avoid any error in execution. I found the input fields where I will need to input the user name and the password and then I have passed the inputs with the send key function. Once these two fields are filled, I just clicked on the login button which I have found using button tag. Removed the flash messages for saving login details and notifications by clicking on \"Not Now\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a356e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver\n",
    "driver.get('https://www.instagram.com/')\n",
    "\n",
    "driver.implicitly_wait(5)\n",
    "#input_box\n",
    "input_box = driver.find_elements(By.TAG_NAME, 'input')\n",
    "\n",
    "inp_userid = input_box[0]\n",
    "inp_userid.send_keys('sample')\n",
    "\n",
    "inp_pswd = input_box[1]\n",
    "inp_pswd.send_keys('sample')\n",
    "\n",
    "login_btn = driver.find_elements(By.TAG_NAME, 'button')\n",
    "login_btn[1].submit()\n",
    "\n",
    "#popup control\n",
    "driver.find_element(By.CLASS_NAME, '_ac8f').click()\n",
    "driver.find_element(By.CLASS_NAME, '_a9_1').click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa8965",
   "metadata": {},
   "source": [
    "Q2. Type for “food” in search bar and print all the names of the Instagram Handles that are displayed in list after typing “food”.\n",
    "\n",
    "Answer: As we logged into account in the previous question script. First I maximized the driver window, as in \"Search\" text is not associated with link in smaller window.  I tried to find the search tab and clicked as to get the search input field using find element(by link text and tag name) and then I clear the input fields so that we are not putting any unwanted value. After that I put only required value and then waited for the list to appear. Once the list appear I have picked all the options that were showing and then I segregated the user_handles with hashtags and location by explore and store to a list. Which I am printing at the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a02c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b260d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tab = driver.find_element(By.LINK_TEXT, 'Search')\n",
    "#search_tab.get_attribute('outerHTML')\n",
    "search_tab.click()\n",
    "search_bar = driver.find_element(By.TAG_NAME, 'input')\n",
    "search_bar.clear()\n",
    "search_bar.send_keys('food')\n",
    "user_ids = driver.find_elements(By.XPATH, \"//div[@style='opacity: 1;']/div[2]/div[2]//a\")\n",
    "food_ids = []\n",
    "#user_ids[1].get_attribute('outerHTML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71741b72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food_tour_ent\n",
      "foodieclicks26\n",
      "satna_foodblooger\n",
      "nepal.food\n",
      "indorefoodexplorer\n",
      "yourfoodlab\n",
      "food\n",
      "food.darzee\n",
      "bhopalfoodwalks\n",
      "indian_tasty_food\n",
      "foods.nepal\n",
      "fun2ooshfood\n",
      "foodie_incarnate\n",
      "foodpharmer\n",
      "svs_food\n",
      "foodie_dil_se__\n",
      "sakshi_food_gallery\n",
      "foodnommics\n",
      "indore_streetfood\n",
      "foodiesfood_court\n",
      "mr.foodie_nepal\n",
      "gwaliorfoodsters\n",
      "being_a_food_vlogger_\n",
      "foodrooaster\n",
      "the_food_icon_mp19\n",
      "food_fun_flock\n",
      "delhifoodwalks\n",
      "mumbaifoodie\n",
      "food_vlogs_and_recipes\n",
      "prateek_thefoodie\n",
      "lazaanafoods\n",
      "__food.fanatic__\n",
      "food_truck27\n",
      "fooditude_bombay\n",
      "foodzeee\n",
      "foodparadise.in\n",
      "food_hunter_cafe_uhr\n",
      "food_reels_fun\n",
      "foodloversmarket\n",
      "foodplazasatna\n",
      "mumbai7merijaan\n",
      "foodtalkindia\n",
      "indori_foodiess\n",
      "foodempowermentproject\n",
      "ashiyana__ourparadise\n",
      "countryfoodcooking\n",
      "sindhifoodlottery\n",
      "meghnasfoodmagic\n",
      "foodnetwork\n"
     ]
    }
   ],
   "source": [
    "for user_id in user_ids:\n",
    "    html_data = user_id.get_attribute('outerHTML')\n",
    "    id_content = BeautifulSoup(html_data, 'html.parser')\n",
    "    name = id_content.a['href'].split('/')[1]\n",
    "    #food_ids.append(name)\n",
    "    #print(name)\n",
    "    if name != 'explore':\n",
    "        food_ids.append(name)\n",
    "        print(name)\n",
    "#driver.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e48bf6b",
   "metadata": {},
   "source": [
    "Q.3 Searching and Opening a profile using -\n",
    "    1.Open profile of “So Delhi” \n",
    "  \n",
    "Answer: Here I started the process after running first two cells of this file. However, we can also follow the above sequence, just by skiping the below cell. The process for input in the search input field is same. Just cleared the existing text in search input field and entered the profile \"So Delhi. Then picked the one which is matching with input and visited the the profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c17952",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()\n",
    "search_tab = driver.find_element(By.LINK_TEXT, 'Search')\n",
    "search_tab.click()\n",
    "search_bar = driver.find_element(By.TAG_NAME, 'input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76709909",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar.clear()\n",
    "search_bar.send_keys('So Delhi')\n",
    "time.sleep(2)\n",
    "id_sodelhi = driver.find_element(By.PARTIAL_LINK_TEXT, 'So Delhi')\n",
    "id_sodelhi.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a32da",
   "metadata": {},
   "source": [
    "Q.4 Follow/Unfollow given handle -\n",
    "    1.Open the Instagram Handle of “So Delhi”\n",
    "    2.Start following it. Print a message if you are already following\n",
    "    3.After following, unfollow the instagram handle. Print a message if you have already unfollowed.\n",
    "  \n",
    "Answer: Now we are on user_handle page now we just need to follow if not already followed. In order to do that, first of all I tried to find the button and I have found the button by class name and then I clicked over it if not follwing. If already following then I am printing that one. Similar process I opting to unfollow, doing that by clicking on the unfollow button then I am confirming it by clicking unfollow on the alter window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0705d243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Following\n"
     ]
    }
   ],
   "source": [
    "follow_btn = driver.find_element(By.CLASS_NAME, '_acan')\n",
    "type_btn = BeautifulSoup(follow_btn.get_attribute('outerHTML'),'html.parser')\n",
    "if type_btn.div.div.text == 'Following':\n",
    "    print('Already Following')\n",
    "else:\n",
    "    follow_btn.click()\n",
    "    print(\"Now Following\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be0002c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Unfollowed\n"
     ]
    }
   ],
   "source": [
    "type_btn = BeautifulSoup(follow_btn.get_attribute('outerHTML'),'html.parser')\n",
    "if type_btn.div.div.text !='Follow':\n",
    "    follow_btn.click()\n",
    "    time.sleep(2) \n",
    "    unfollow = driver.find_element(By.XPATH, '//div[contains(@class, \"xxfnqb6\")]/div/div/div/div[8]')\n",
    "    unfollow.click()\n",
    "    print(\"Now Unfollowed\")\n",
    "else:\n",
    "    print('Already Unfollowed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc645b",
   "metadata": {},
   "source": [
    "Q.5 Like/Unlike posts - \n",
    "    1. Liking the top 30 posts of the ‘dilsefoodie'. Print message if you have already liked it.\n",
    "    2. Unliking the top 30 posts of the ‘dilsefoodie’. Print message if you have already unliked it.\n",
    "\n",
    "Answer: Here I have created one function which is handling like and unlike of recent 30 posts. The function takes one argument i.e likes or unlikes, based on choice to like or dislike the post and it does for top 30 posts. If any post is already liked or unliked then it prints that the following action which we want to do has already been done.I used BeautifulSoup along with selenium to locate like & next button and to know wheather the post is already liked or not. Process of searching the handle is same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb950ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()\n",
    "search_tab = driver.find_element(By.LINK_TEXT, 'Search')\n",
    "search_tab.click()\n",
    "search_bar = driver.find_element(By.TAG_NAME, 'input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a991ca28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_bar.clear()\n",
    "search_bar.send_keys('dilsefoodie')\n",
    "id_dilsefoodie = driver.find_element(By.PARTIAL_LINK_TEXT, 'dilsefoodie')\n",
    "id_dilsefoodie.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe8446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def like_unlike_post(choose):\n",
    "    post = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH,\"//div[@class= '_ac7v  _al3n']/div/a\")))\n",
    "    post.click()\n",
    "    i = 1\n",
    "    while True:\n",
    "        like_btn = driver.find_element(By.XPATH,\"//article[@class= '_aatb _aate _aatg _aati']/div/div[2]//section//span//span\")\n",
    "        like_type = BeautifulSoup(like_btn.get_attribute('innerHTML'),'html.parser').find('svg')\n",
    "        \n",
    "        if like_type['aria-label'].strip().lower()==choose.strip().lower():\n",
    "            like_btn.click()\n",
    "        else:\n",
    "            print('Already {}d'.format(choose))\n",
    "        time.sleep(2)\n",
    "        next_post = driver.find_elements(By.XPATH,\"//button[@class='_abl-']\")\n",
    "        if i == 1:\n",
    "            next_btn = BeautifulSoup(next_post[0].get_attribute('innerHTML'),'html.parser').find('svg')\n",
    "            if next_btn['aria-label'] == 'Next':\n",
    "                next_post[0].click()\n",
    "        else:\n",
    "            next_btn = BeautifulSoup(next_post[1].get_attribute('innerHTML'),'html.parser').find('svg')\n",
    "            if next_btn['aria-label'] == 'Next':\n",
    "                next_post[1].click()\n",
    "                \n",
    "        i+=1\n",
    "        time.sleep(3)\n",
    "        if i==31:\n",
    "            print(\"{}d 30 recent posts\".format(choose.title()))\n",
    "            driver.back()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1177785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liked 30 recent posts\n"
     ]
    }
   ],
   "source": [
    "# Like 30 Posts of dilsefoodie\n",
    "like_unlike_post(\"like\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fcb38f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already unliked\n",
      "Unliked 30 recent posts\n"
     ]
    }
   ],
   "source": [
    "# Unlike  30 Posts of dilsefoodie\n",
    "like_unlike_post(\"unlike\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01421a65",
   "metadata": {},
   "source": [
    "Q.6 Extract list of followers - \n",
    "    1. Extract the usernames of the first 500 followers of ‘foodtalkindia’ and ‘sodelhi’. \n",
    "    2. Now print all the followers of “foodtalkindia” that you are following but those who don’t follow you.\n",
    "    \n",
    "Answer: Here I used three functions one to search page,second to scroll and load followers list and final to extract avialable followers(as all followers of varified account are not avilable to be viewed by other users in instagram). Then used them to extract followers of 'sodelhi', 'foodtalkindia' and also my follower to print all the followers of “foodtalkindia” that I am following but those who don’t follow me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e29646ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchPage(page_name):\n",
    "    search_tab = driver.find_element(By.LINK_TEXT, 'Search')\n",
    "    search_tab.click() \n",
    "    search_bar = driver.find_element(By.TAG_NAME, 'input')\n",
    "    time.sleep(2)\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(page_name)\n",
    "\n",
    "def load_all():\n",
    "    elem = driver.find_element(By.XPATH, '//div[@class=\"_aano\"]/div[@style=\"height: auto; overflow: hidden auto;\"]')\n",
    "    i = 0\n",
    "    flag = True\n",
    "    current_height = int(elem.get_attribute(\"scrollHeight\"))\n",
    "    while flag:\n",
    "        if i == current_height:\n",
    "            flag = False\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(0, arguments[1]);\", elem,  current_height)\n",
    "        time.sleep(8)\n",
    "        element = driver.find_element(By.XPATH, '//div[@class=\"_aano\"]/div[@style=\"height: auto; overflow: hidden auto;\"]')\n",
    "        i = current_height\n",
    "        current_height = int(element.get_attribute(\"scrollHeight\"))\n",
    "    \n",
    "\n",
    "def Ext_first500followers():\n",
    "    global followers\n",
    "    global followers_ifollow\n",
    "    followers = []\n",
    "    followers_ifollow = []\n",
    "    try:\n",
    "        follower_btn = driver.find_element(By.PARTIAL_LINK_TEXT, 'Followed by')\n",
    "        follower_btn.click()\n",
    "        driver.find_element(By.LINK_TEXT, 'See all followers').click()\n",
    "        time.sleep(5)\n",
    "        load_all()\n",
    "        #driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        follower_data = driver.find_elements(By.XPATH, '//div[@class = \"_aano\"]/div[@style=\"height: auto; overflow: hidden auto;\"]/div/div')\n",
    "        p = 1\n",
    "        for i in follower_data:\n",
    "            if p > 500:\n",
    "                break\n",
    "            html_data = BeautifulSoup(i.get_attribute('innerHTML'),'html.parser')\n",
    "            name = html_data.a['href'].strip().split('/')[1]\n",
    "            know = html_data.button.text\n",
    "            if know =='Following':\n",
    "                followers_ifollow.append(name)\n",
    "            p += 1\n",
    "            followers.append(name)\n",
    "    except:\n",
    "        follower_btn = driver.find_element(By.PARTIAL_LINK_TEXT, 'followers')\n",
    "        follower_btn.click()\n",
    "        driver.find_element(By.LINK_TEXT, 'See all followers').click()\n",
    "        time.sleep(5)\n",
    "        load_all()\n",
    "        follower_data = driver.find_elements(By.XPATH, '//div[@class = \"_aano\"]/div[@style=\"height: auto; overflow: hidden auto;\"]/div/div')\n",
    "        p = 1\n",
    "        for i in follower_data:\n",
    "            if p > 500:\n",
    "                break\n",
    "            html_data = BeautifulSoup(i.get_attribute('innerHTML'),'html.parser')\n",
    "            name = html_data.a['href'].strip().split('/')[1]\n",
    "            know = html_data.button.text\n",
    "            if know =='Following':\n",
    "                followers_ifollow.append(name)\n",
    "            p += 1\n",
    "            followers.append(name)\n",
    "    return followers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426d5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()\n",
    "driver.get('https://www.instagram.com/')\n",
    "SearchPage('sodelhi') \n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba0e4799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['keshavsadhna', 'rajputikangan', 'rjkisnaa', 'arshadqureshi2744', 'chhillar4147', 'siddhartha_pandey17_18', 'myena_thrift_shop', 'oyepreeta', 'hashima_goyal', 'anubha.shakya', 'nikhil.shawarma', 'highonlife_______', 'pri_shanthi', 'thesanskritirawat', 'smita.1414', 'zensince2006', 'i_kj24', 'flightsanddreams', 'jus.saleha', 'priyankajain0908', 'maninder.068', 'kaur.man1992', 'the.manas.sri', 'iaman.mr', 'ankahi_baatee__', 'ruqayaaaaaa0', 'gauravsilot', 'akanshaa_singh1999', 'in_dia2579', 'neharikakapur', 'deeksha_chak', 'deepaljain10', 'graceful_preenja', '47nyx', 'khanarham1215', 'kundra5914', 'siddharthmahajan05', 'ronn81099', 'ianshul1999', 'respectforsquats', 'verycollectivestudio', 'greysasquatch', 'prerna.s9', 'navdeep_ggg', 'salty2us', 'tandon.saloni14', 'edwin9.3', 'rahulpandeyofficial14', 'jaipeera587', 'delhi_malayalee', 'notafafjamal', 'aar.av4492', 'ritika_b07', 'vanikakohli_20', 'naveenverma___', 'roman_ss3', 's.i.n.g.h.m.a.n.d.y', 'virajsingh89', 'the.dreamswedding', 'silvamayura', 'sattwadoodle', 'smriti_sood_', 'd_path_ak', 'nibhrati_soni', 'nago.ri5811', 'psyched_lens', 'hiimanshu.aggarwal', 'pranjalmunjal', 'riya.r__', 'indian_hiker', '_justt.divya__', 'lakshaysardana_', 'vikrant_1912', 'thisisrajdeep_18', 'pranjaliaggarwal0923', 'lakshayy_7703', 'sweek_aar', 'meenasumit3278', '_explore_with__himanshu', 'thakurpriyaankachauhan', 'superbrightok', 'amitbudhwar31323', 'saumyashukla2707', 'khan_pathan__56', 'saxena_sameeer', 'shivani__rai', 'amitranaaaa', 'xary_anx123', 'anuradha2365', 'iamvaruntiwari', 'goel_nakul_08', 'aggarwalsahil08', 'salmanizuber26', 'sai_sashikala', 'rajdev.kumar.7758235', 'faizan6ix', 'sercanakgol', 'iamdeepanshu.raj', 'rl_jyotinegi', 'assnavii', '_go.withtheflow', 'dillikiladkiyan', 'ish_aa_an', 'aashu_s0663', 'disha_singh443', 'lifeq_outes5', 'shuklasaumya0131', 'kaustubh_prasad', 'shan.a8598', 'saumyayaya19', 'freedom_oflight', 'banjara_on_wheels', 'jyoti478_agrawal', 'rajatkatiyar2002', '__shilpiiiii__', 'anuragmishra1854', 'naseemehfooz', 'kritirajputtt', 'sumeetsinha', 'abhijeetsingh5026', 'kashishh10__', 'aditigarg7921', 'alien.brain0', 'govind__bhejo963', 'kalpana566', 'ashishsingh.x', 'samairakalra26', 'govindageetika', 'meghanshagupta._', 'ritvik_warrior', 'jasswantsanskriti', 'yogeshdalal014', 'bhavya.marwah', '_for.you.by.us_', 'hamzaqureshi_o7', 'abhimanyusingharora', 'puipuilaizo', 'amishijaiswal', 'dentalcentredwarka', 'yaman_bansal_', 'jn_suman92', 'natasha_sahrma1235799', 'drrahulnogiya', '0illusion_', 'salali5041', 'gautamv29', 'true_solivagant', 'anshulika__', 'honey__singh1001', 'khushisaini2112', 'a.n.v.i.t.a', 'lipinayak64', 'kirorimal_alumni_official_', 'manpreetsinghroop', 'rishabhgogna', 'nishtha.chaudharyy', 'koulpriyanka2020', '_kartikmalhotra_', 'alpita_kamra', '_sarahfatima_', 'wonderladie_s', 'viibhutiii', 'paawxni.xoxo', 'nirmalpaulose', 'kashish_701', 'aman.jain2641', 'anchordims', 'kaif_khan_bikaner_', 'itsjivii', 'ak169657', 'sanatani._.yug', 'sarthak00018', 'viipiinkumar', 'aakhya_div', 'rankil_bo', 'farheensiddiqui130', 'bhumidutt06', 'bhavikap02', 'pujitapandey', 'mannmaujeeee', 'riya._.1302', 'arorageetanzali', 'priyanka_physio.89', 'rioandbeby', '_.parikaaa', 'creativejojo1507', '___akankshaaaa______', 'the_aviation_geek17', 'ahana_reigns_pvt', 'namitasaharan77', 'its_shreyaasingh', 'shivam__garg_', 'pal_arun777', 'zoyakhan_0000__', 'stuti_goyal', 'kratikachauhan01', 'khuranashefali', '_swati_sood_', '_durvaa.a', 'debalina_sh', 'being__rajat_', 'sunvivek2003', 'ahluwalia.rahul', 'menkamalhotra', '_akhilesh_bhatt_', '_akankshapant', 'nikitachaudhary576', 'anurag_verma_29', 'aditi1912__', 'prachimajila', 'itzzainulhere', 'suhailmalik8297', 'jahnaviieeee', '_farjita', 'think_roz', 'its_yuvisapra', 'b.abhinav_', 'suhailpashaa', 'majestic_shivii', 'amit_krprajapati', 'epohuoywonk', 'rishabh.sheoran', 'vodkashotsonly', 'dheerajhimself', 'mwmw.0.0', 'urbn_desi', 'priyanka.narang_', '_m.a.a.n.v.i_', 'ansh.vachher', 'mehtabali1573', 'sangeet1961', 'amitpathak8636', 'clementperruche', 'siimaab_', 'chaitanya__srivastava', 'rickeykohli', 'naushad_official8', 'insane_pradeep', 'yup_thatformerchild', 'ladi.walia', '_s_h_i_i_v_u', 'binakohli', 'sambhawnathakur', 'bholi__si__surat', 'idkman0111', '___n.o.t.e.s', 'prarit__sharmaa', 'sarthakk19', 'juliaakk920', 'athira4415_', 'is.vivacious', 'jagriti5718', 'ishiitajindal_', 'mrastogi_2203', 'suhaansaluja', 'kirtiyaduvanshi0', 'dedalusbo', 'bhuvnesh_bawra', 'singhal5529', 'cultu.re07', 'anormalpieceofshit', 'suitsyou.boutique', 'amananand5366', 'singhaniadeepti', '_the_trouble_magnet', '_riyajha', 'darshgupta38', 'pratikshaabhatia', 'nityanayra8', 'devika.sharma1103', 'harshita_shrma.2575', 'brain_hacker746', 'dichadelhi', 'boogeyman8931', 'aditya._.periwal', 'kanika_mttl', '_venus', 'damon_salvatore_1107', 'priya.dhawan123', 'zoyaamir98', 'manishajain2112', 'senogal_', 'arsh_gaur', '_mittalriya', 'vidit.garg.0606', 'vershabhanu', 'titikshatewari', 'naazamra', 'iamrajvaibhav', 'gracefulspirit_', 'prateekznmd', 'zubiyawaseem0', 'shipra.thakur.288', 'saksham_.7369', 'ashishkumarrsingh', 'sneha_jaiswal2609', 'anshusharma51', 'dkaur93', '___aestrophile___blue_king', 'the_name_is_befikra', 'vanshii.pvttt_', 'aadya__11']\n"
     ]
    }
   ],
   "source": [
    "id_ = driver.find_element(By.PARTIAL_LINK_TEXT, 'sodelhi')\n",
    "id_.click()\n",
    "time.sleep(3)\n",
    "followers = []\n",
    "Ext_first500followers()\n",
    "print(followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dbbac97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kushakapila', 'ankushbahuguna', 'rochakkohli', 'shubham3011s', 'glensiajohn8', '21gunsalutecateringservices', 'aashnamehta99', 'aditya2413', 'lakhwara.akanksha', 'akriti7508', 'anvitamaurya', 'ashishkumarrsingh', 'simply_lavish_bites', 'billionpreet.delhi', '___mr___deepak_rajpoot___302', 'dhinukaboralessa', 'doc_a_tale', 'frdeen.khan.501151', 'goraksh1387', 'haldiram_in_la', 'happyfoodplate', 'honey__singh1001', 'pc2go_2397_', 'kriticism101', 'flash_station', 'nainamahajan', 'nehageraaa', 'nidhiisharma_4', 'chefpradeepbhandari', 'pranavajgaonkar', 'pree_tjeet13', '_rajdeep_bhullar_', 'rk1786012', 'kochharrakhi', 'yadav2151ravi', 'rjivan_ali_7548', 'sagarkohhli', 'shahbajkoushar', 'shantanuraizada', 'notyourshivangisaxena', 'siddie_aditya', 'muddykanna', 'organic.sunrise.natural', 'techpiefood', 'theholygathering', 'nandini_8989', 'wayfarer_im', 'vijaitha_mr', 'vijaoi', 'nick4devil', 'umairjavaid15', 'imranstylingfilms', 'i_am_roshan_avhad']\n"
     ]
    }
   ],
   "source": [
    "driver.get('https://www.instagram.com/')\n",
    "SearchPage('foodtalkindia') \n",
    "time.sleep(5)\n",
    "id_ = driver.find_element(By.PARTIAL_LINK_TEXT, 'foodtalkindia')\n",
    "id_.click()\n",
    "time.sleep(3)\n",
    "Ext_first500followers()\n",
    "foodtalkindia_followers_ifollow = followers_ifollow\n",
    "print(followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fee7a246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kushakapila\n",
      "ankushbahuguna\n",
      "rochakkohli\n"
     ]
    }
   ],
   "source": [
    "driver.get('https://www.instagram.com/')\n",
    "my_followers = []\n",
    "follower_btn = driver.find_element(By.LINK_TEXT, 'Profile')\n",
    "follower_btn.click()\n",
    "follower_btn = driver.find_element(By.PARTIAL_LINK_TEXT, 'follower')\n",
    "follower_btn.click()\n",
    "load_all()\n",
    "follower_data = driver.find_elements(By.XPATH, '//div[@class = \"_aano\"]/div[@style=\"height: auto; overflow: hidden auto;\"]/div/div')\n",
    "for i in follower_data:\n",
    "    html_data = BeautifulSoup(i.get_attribute('innerHTML'),'html.parser')\n",
    "    name = html_data.a['href'].strip().split('/')[1]\n",
    "    followers.append(name)\n",
    "    \n",
    "for i in foodtalkindia_followers_ifollow:\n",
    "    if not(i in my_followers):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475e1dc",
   "metadata": {},
   "source": [
    "Q. 7 Check the story of ‘coding.ninjas’. Consider the following Scenarios and print error messages accordingly -\n",
    "     If You have already seen the story,\n",
    "     Or The user has no story,\n",
    "     Or View the story if not yet seen.\n",
    "     \n",
    "Answer: Here I have used try and except (exception method) which will return one string out of these three strings--\n",
    "\n",
    "     1. \"You have not seen the story yet! The story will be shown to you now in the driver window\",    \n",
    "     2. \"You have already seen the story!\",or    \n",
    "     3. \"The user has no story!\" \n",
    "   based on the conditions.                  \n",
    "It first tries to find canvas tag and its height. If the height is 210, the story is not seen yet, else if it is 208, the story has already seen. But if it throws \"NoSuchElementException\" there is no canvas tag that is no story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f535cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchPage(page_name):\n",
    "    search_tab = driver.find_element(By.LINK_TEXT, 'Search')\n",
    "    search_tab.click() \n",
    "    search_bar = driver.find_element(By.TAG_NAME, 'input')\n",
    "    time.sleep(2)\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(page_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a5c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()\n",
    "driver.get('https://www.instagram.com/')\n",
    "SearchPage('coding.ninja') \n",
    "time.sleep(5)\n",
    "id_ = driver.find_element(By.PARTIAL_LINK_TEXT, 'coding.ninja')\n",
    "id_.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488b0e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have already seen the story!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if int(driver.find_element(By.XPATH,'//main//div[contains(@class, \"_aarf\")]/canvas').get_attribute('height'))==210:\n",
    "        print('You have not seen the story yet! The story will be shown to you now in the driver window')\n",
    "        driver.find_element(By.XPATH,'//main//div[contains(@class, \"_aarf\")]').click()\n",
    "    elif int(driver.find_element(By.XPATH, '//main//div[contains(@class, \"_aarf\")]/canvas').get_attribute('height'))==208:\n",
    "        print('You have already seen the story!')\n",
    "except NoSuchElementException:\n",
    "    print('The user has no story!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1432506",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
